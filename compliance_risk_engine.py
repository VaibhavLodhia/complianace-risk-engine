"""
Two-Stage Predictive Data Governance and Compliance Risk Engine

This script implements:
1. Loading data from CSV files (generated by generate_data.py)
2. Isolation Forest for anomaly detection
3. XGBoost classifier for compliance violation prediction

Note: Run generate_data.py first to create the required CSV files.
"""

import os
import warnings

import pandas as pd
import xgboost as xgb
from sklearn.ensemble import IsolationForest
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

warnings.filterwarnings("ignore")

print("=" * 80)
print("Predictive Data Governance and Compliance Risk Engine")
print("=" * 80)
print()

# ============================================================================
# PART 1: LOAD DATA FROM CSV FILES
# ============================================================================
print("Part 1: Loading Data from CSV Files...")
print("-" * 80)

# Check if data files exist
metadata_path = "data/metadata.csv"
access_logs_path = "data/access_logs.csv"

if not os.path.exists(metadata_path) or not os.path.exists(access_logs_path):
    print("ERROR: Data files not found!")
    print(f"  Missing: {metadata_path}")
    print(f"  Missing: {access_logs_path}")
    print()
    print("Please run generate_data.py first to create the data files.")
    print("  python generate_data.py")
    exit(1)

# Load metadata
df_metadata = pd.read_csv(metadata_path)
df_metadata["PHI_Flag"] = df_metadata["PHI_Flag"].astype(bool)

print(f"Loaded df_metadata: {len(df_metadata)} assets")
print(f"  - PHI assets: {df_metadata['PHI_Flag'].sum()}")
print(f"  - Plain text assets: {(df_metadata['Encryption_Status'] == 'Plain').sum()}")

# Load access logs
df_access_logs = pd.read_csv(access_logs_path, parse_dates=["Timestamp"])
df_access_logs["PHI_Flag"] = df_access_logs["PHI_Flag"].astype(bool)
df_access_logs["Policy_Violation"] = df_access_logs["Policy_Violation"].astype(int)

print(f"Loaded df_access_logs: {len(df_access_logs)} rows")
print(
    f"Policy violations in data: {df_access_logs['Policy_Violation'].sum()} out of {len(df_access_logs)}"
)
print(f"Violation rate: {df_access_logs['Policy_Violation'].mean():.2%}")
print()

# ============================================================================
# PART 2: ANOMALY DETECTION (ISOLATION FOREST)
# ============================================================================
print("Part 2: Anomaly Detection with Isolation Forest...")
print("-" * 80)

# Preprocessing: Create time-based features
df_access_logs["Hour_of_Day"] = df_access_logs["Timestamp"].dt.hour
df_access_logs["Day_of_Week"] = df_access_logs["Timestamp"].dt.dayofweek
df_access_logs["Day_of_Month"] = df_access_logs["Timestamp"].dt.day
df_access_logs["Month"] = df_access_logs["Timestamp"].dt.month

# One-hot encode categorical features
categorical_cols = ["Access_Type", "IP_Location"]
df_encoded = pd.get_dummies(df_access_logs[categorical_cols], prefix=categorical_cols)

# Combine features for Isolation Forest
features_for_if = pd.concat(
    [
        df_access_logs[["Hour_of_Day", "Day_of_Week", "Day_of_Month", "Month"]],
        df_encoded,
    ],
    axis=1,
)

print(f"Features for Isolation Forest: {features_for_if.shape[1]} features")
print(f"Feature names: {list(features_for_if.columns)}")

# Train Isolation Forest
print("Training Isolation Forest model...")
isolation_forest = IsolationForest(
    n_estimators=100,
    contamination=0.025,  # Expect ~2.5% anomalies
    random_state=42,
    n_jobs=-1,
)

isolation_forest.fit(features_for_if)

# Generate anomaly scores
anomaly_scores = isolation_forest.decision_function(features_for_if)
# Convert to positive scale (higher = more anomalous)
df_access_logs[
    "Anomaly_Score"
] = -anomaly_scores  # Negate because IF returns negative for anomalies

print(
    f"Anomaly scores generated. Range: [{df_access_logs['Anomaly_Score'].min():.4f}, {df_access_logs['Anomaly_Score'].max():.4f}]"
)
print(f"Mean anomaly score: {df_access_logs['Anomaly_Score'].mean():.4f}")
print()

# ============================================================================
# PART 3: COMPLIANCE VIOLATION CLASSIFIER (XGBOOST)
# ============================================================================
print("Part 3: Compliance Violation Classifier with XGBoost...")
print("-" * 80)

# Feature Integration: Join Anomaly_Score with metadata features
df_features = df_access_logs[
    [
        "Data_Asset_ID",
        "Anomaly_Score",
        "Hour_of_Day",
        "Day_of_Week",
        "Day_of_Month",
        "Month",
        "Policy_Violation",
        "Access_Type",
        "IP_Location",
    ]
].copy()

# Merge with metadata
df_features = df_features.merge(
    df_metadata[["Data_Asset_ID", "PHI_Flag", "Encryption_Status"]],
    on="Data_Asset_ID",
    how="left",
)

# One-hot encode categorical features
df_metadata_encoded = pd.get_dummies(
    df_features[["Access_Type", "IP_Location", "Encryption_Status"]],
    prefix=["Access_Type", "IP_Location", "Encryption_Status"],
)

# Convert PHI_Flag to binary
df_features["PHI_Flag"] = df_features["PHI_Flag"].astype(int)

# Combine all features
X = pd.concat(
    [
        df_features[
            [
                "Anomaly_Score",
                "Hour_of_Day",
                "Day_of_Week",
                "Day_of_Month",
                "Month",
                "PHI_Flag",
            ]
        ],
        df_metadata_encoded,
    ],
    axis=1,
)

y = df_features["Policy_Violation"]

print(f"Final feature set shape: {X.shape}")
print(f"Feature names: {list(X.columns)}")
print()

# Split into train and test sets (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")
print()

# Train XGBoost Classifier
print("Training XGBoost Classifier...")
xgb_classifier = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    eval_metric="logloss",
    use_label_encoder=False,
)

xgb_classifier.fit(X_train, y_train)

# Predict probabilities
y_pred = xgb_classifier.predict(X_test)
y_pred_proba = xgb_classifier.predict_proba(X_test)[:, 1]  # Probability of violation

# Add predictions to test set
df_test_results = X_test.copy()
df_test_results["Policy_Violation_Actual"] = y_test.values
df_test_results["Policy_Violation_Predicted"] = y_pred
df_test_results["Compliance_Infraction_Score"] = y_pred_proba

# Add original columns for display
test_indices = X_test.index
df_test_results["Timestamp"] = df_access_logs.loc[test_indices, "Timestamp"].values
df_test_results["User_ID"] = df_access_logs.loc[test_indices, "User_ID"].values
df_test_results["Data_Asset_ID"] = df_access_logs.loc[
    test_indices, "Data_Asset_ID"
].values
df_test_results["Access_Type"] = df_access_logs.loc[test_indices, "Access_Type"].values
df_test_results["IP_Location"] = df_access_logs.loc[test_indices, "IP_Location"].values
df_test_results["Anomaly_Score"] = df_access_logs.loc[
    test_indices, "Anomaly_Score"
].values

print("XGBoost training completed!")
print()

# ============================================================================
# FINAL OUTPUT
# ============================================================================
print("=" * 80)
print("FINAL RESULTS")
print("=" * 80)
print()

# Classification Report
print("Classification Report:")
print("-" * 80)
print(classification_report(y_test, y_pred, target_names=["Normal", "Violation"]))
print()

# Top 5 records with highest Compliance Infraction Score
print("Top 5 Records with Highest Compliance Infraction Score:")
print("-" * 80)
top_5 = df_test_results.nlargest(5, "Compliance_Infraction_Score")[
    [
        "Timestamp",
        "User_ID",
        "Data_Asset_ID",
        "Access_Type",
        "IP_Location",
        "Anomaly_Score",
        "Compliance_Infraction_Score",
        "Policy_Violation_Actual",
        "Policy_Violation_Predicted",
    ]
]

pd.set_option("display.max_columns", None)
pd.set_option("display.width", None)
pd.set_option("display.max_colwidth", None)
print(top_5.to_string(index=False))
print()

print("=" * 80)
print("Analysis Complete!")
print("=" * 80)
